{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Scraping domain.com.au"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will scrape the website domain.com.au to extract different features of current rental properties including rental price, number of bedrooms, bathrooms, parkings, suburb and postcode, etc. The URL links of each property will also be recorded for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Import libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%run ../scripts/'scrape domain.com.py'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Start scraping website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the first page of property listings\n",
    "url = \"https://www.domain.com.au/rent/melbourne-region-vic/?excludedeposittaken=1&page=1\"\n",
    "properties = []\n",
    "page_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin page 1\n",
      "Begin page 2\n",
      "Begin page 3\n",
      "Begin page 4\n",
      "Begin page 5\n",
      "Begin page 6\n",
      "Begin page 7\n",
      "Begin page 8\n",
      "Begin page 9\n",
      "Begin page 10\n",
      "Begin page 11\n",
      "Begin page 12\n",
      "Begin page 13\n",
      "Begin page 14\n",
      "Begin page 15\n",
      "Begin page 16\n",
      "Begin page 17\n",
      "Begin page 18\n",
      "Begin page 19\n",
      "Begin page 20\n",
      "Begin page 21\n",
      "Begin page 22\n",
      "Begin page 23\n",
      "Begin page 24\n",
      "Begin page 25\n",
      "Begin page 26\n",
      "Begin page 27\n",
      "Begin page 28\n",
      "Begin page 29\n",
      "Begin page 30\n",
      "Begin page 31\n",
      "Begin page 32\n",
      "Begin page 33\n",
      "Begin page 34\n",
      "Begin page 35\n",
      "Begin page 36\n",
      "Begin page 37\n",
      "Begin page 38\n",
      "Begin page 39\n",
      "Begin page 40\n",
      "Begin page 41\n",
      "Begin page 42\n",
      "Begin page 43\n",
      "Begin page 44\n",
      "Begin page 45\n",
      "Begin page 46\n",
      "Begin page 47\n",
      "Begin page 48\n",
      "Begin page 49\n",
      "Begin page 50\n"
     ]
    }
   ],
   "source": [
    "while url:  # stop when there are no more pages to scrape\n",
    "    print(\"Begin page\", page_count)    \n",
    "\n",
    "    # Fetch and parse HTML of the current page\n",
    "    soup = get_soup(url)    # function from scrape domain.com\n",
    "\n",
    "    # Get a list of all property listings on the page\n",
    "    listings = soup.find_all('div', {'class': 'css-qrqvvg'})\n",
    "\n",
    "    # Extract information of each property\n",
    "    for property in listings:\n",
    "        price = get_price(property)\n",
    "        if not price:   # exclude properties that do not have a rental price         \n",
    "            continue     # usually are properties under application\n",
    "\n",
    "        suburb, postcode = get_suburb(property)\n",
    "        bedrooms, bathrooms, parkings = get_features(property)\n",
    "        property_type = property.find('span', class_='css-693528').text.strip()     \n",
    "\n",
    "        # Look into the property's page to find additional features\n",
    "        link = property.find('a', href = True)\n",
    "        property_url = link['href']\n",
    "        property_soup = get_soup(property_url)\n",
    "\n",
    "        additional_features = get_additional_features(property_soup)\n",
    "        \n",
    "        # Store the extracted data in a dictionary\n",
    "        property_data = {\n",
    "            'price (AUD per week)': price,\n",
    "            'bedrooms': bedrooms,\n",
    "            'bathrooms': bathrooms,\n",
    "            'parkings': parkings,\n",
    "            'property type': property_type,\n",
    "            'suburb': suburb,\n",
    "            'postcode': postcode,\n",
    "            'additional features': additional_features,\n",
    "            'property url': property_url\n",
    "        }\n",
    "\n",
    "        # Append the dictionary to the list of properties\n",
    "        properties.append(property_data)\n",
    "    url = get_next_url(soup)\n",
    "    page_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to properties.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(properties)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('../data/landing/properties.csv', index=False)\n",
    "\n",
    "print('Data successfully written to properties.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
